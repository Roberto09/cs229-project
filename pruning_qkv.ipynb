{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/daniel/miniconda3/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import os\n",
    "from dataset_preprocessing import TokenInfo\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "\n",
    "model_id = \"microsoft/phi-1_5\"\n",
    "model_revision = \"349cf8b5e81fd5f791d1740da5de1313a0419bbd\" # latest as of feb 1st\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    revision=model_revision,\n",
    "    trust_remote_code=True,\n",
    "    # be careful with this?\n",
    "    # torch_dtype=torch.float16,\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_modules(model):\n",
    "    layers = model.get_submodule(\"model\").get_submodule(\"layers\")\n",
    "\n",
    "    mlps = [layer.get_submodule(\"mlp\") for layer in layers]\n",
    "    fc1s = [mlp.fc1 for mlp in mlps]\n",
    "    fc2s = [mlp.fc2 for mlp in mlps]\n",
    "\n",
    "    attns = [layer.get_submodule(\"self_attn\") for layer in layers]\n",
    "    q_projs = [attn.q_proj for attn in attns]\n",
    "    k_projs = [attn.k_proj for attn in attns]\n",
    "    v_projs = [attn.v_proj for attn in attns]\n",
    "    denses = [attn.dense for attn in attns]\n",
    "\n",
    "    return [q_projs, k_projs, v_projs, denses, fc1s, fc2s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(logits, labels, model):\n",
    "    \"\"\" Returns crossentropy loss per token, w/o reduction \"\"\"\n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    orig_shape = shift_labels.shape\n",
    "    # Flatten the tokens\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "    shift_logits = shift_logits.view(-1, model.config.vocab_size)\n",
    "    shift_labels = shift_labels.view(-1)\n",
    "    # Enable model parallelism\n",
    "    shift_labels = shift_labels.to(shift_logits.device)\n",
    "    loss = loss_fct(shift_logits, shift_labels).view(orig_shape)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc_grad(model, examples, modules):\n",
    "    \"\"\" Computes squared gradient term in delta loss approximation.\n",
    "    Then it stores it in the param.acc_grad attribute.\"\"\"\n",
    "    params_q = [list(module.parameters()) for module in modules[0]]\n",
    "    params_q = itertools.chain.from_iterable(params_q)\n",
    "    params_k = [list(module.parameters()) for module in modules[1]]\n",
    "    params_k = itertools.chain.from_iterable(params_k)\n",
    "    params_v = [list(module.parameters()) for module in modules[2]]\n",
    "    params_v = itertools.chain.from_iterable(params_v)\n",
    "    params_d = [list(module.parameters()) for module in modules[3]]\n",
    "    params_d = itertools.chain.from_iterable(params_d)\n",
    "    params_fc1s = [list(module.parameters()) for module in modules[4]]\n",
    "    params_fc1s = itertools.chain.from_iterable(params_fc1s)\n",
    "    params_fc2s = [list(module.parameters()) for module in modules[5]]\n",
    "    params_fc2s = itertools.chain.from_iterable(params_fc1s)\n",
    "                \n",
    "    all_params = params_q + params_k + params_v + params_d + params_fc1s + params_fc2s\n",
    "                \n",
    "    res = model(examples, labels=examples)\n",
    "    losses_tens = custom_loss(res.logits, examples, model)\n",
    "    losses = [loss.mean() for loss in losses_tens]\n",
    "                        \n",
    "    # import pdb; pdb.set_trace()\n",
    "    for example_loss in losses:\n",
    "        example_loss.backward(retain_graph=True)\n",
    "        for param in all_params: # for all the weights\n",
    "            num_examples = examples.shape[0]\n",
    "            with torch.no_grad():\n",
    "                grad = param.grad.detach()\n",
    "                sq_grad = grad * grad / num_examples\n",
    "                if hasattr(param, \"acc_grad\"):\n",
    "                    param.acc_grad += sq_grad\n",
    "                else:\n",
    "                    param.acc_grad = sq_grad\n",
    "        model.zero_grad()\n",
    "        del example_loss\n",
    "        torch.cuda.empty_cache()\n",
    "    return losses_tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_paired_importance(modules1, modules2, dim1=1, dim2=0):\n",
    "    \"\"\"Shared function for mlp/q/k/v/d. Given modules with gradients and squared gradients stored,\n",
    "    approximated the importances as the delta of the loss using taylor\n",
    "    expansion.\"\"\"\n",
    "    importances = []\n",
    "\n",
    "    pairs = list(zip(modules1, modules2))\n",
    "        \n",
    "    for pair in pairs:\n",
    "\n",
    "        m1 = pair[0]\n",
    "        m2 = pair[2]\n",
    "\n",
    "        salience_w1 = m1.weight * m1.weight.grad\n",
    "        salience_w2 = m1.weight * m2.weight.grad\n",
    "        \n",
    "        salience_w1 = salience_w1 - 0.5 * m1.weight * m1.weight.acc_grad * m1.weight\n",
    "        salience_w2 = salience_w2 - 0.5 * m2.weight * m2.weight.acc_grad * m2.weight\n",
    "\n",
    "        importance_w1_component = salience_w1.abs().sum(dim=dim1)\n",
    "        importance_w2_component = salience_w2.abs().sum(dim=dim2)\n",
    "\n",
    "        importance = importance_w1_component + importance_w2_component\n",
    "        importances.append(importance.detach().cpu())\n",
    "    return importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input(storage, key):\n",
    "    \"\"\"\n",
    "    Get input into mlp layer in forward pass\n",
    "    \"\"\"\n",
    "    def hook(module, input, output):\n",
    "        # Assuming the layer takes a single Tensor as input, store it\n",
    "        storage[key] = input[0].detach()\n",
    "    return hook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_input_trailing_token(mlp_input_dict):\n",
    "    \"\"\"\n",
    "    Get single embedding(inputs into mlps) for each layer\n",
    "    for trailing token in sequence\n",
    "    \"\"\"\n",
    "\n",
    "    # alwyas take first sequence, is randomly sampled beforehand, so no need to random sample again\n",
    "    # importances are based on the entire batch, but we only take one sample input\n",
    "    return [inputs[0, -1, :].cpu() for layer, inputs in mlp_input_dict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_delta_loss_importances(model, examples):\n",
    "    \"\"\"Computes and returns impotances of every hidden neuron in the model's\n",
    "    mlps. Here we define importance as the change of the loss if we were to\n",
    "    set the inbound and outbound weights of a neuron to 0.\"\"\"\n",
    "    \n",
    "    qs, ks, vs, ds, fc1s, fc2s = get_all_modules(model)\n",
    "\n",
    "    mlp_input_dict = {}\n",
    "    hooks = []\n",
    "\n",
    "    for i, fc1 in enumerate(fc1s):\n",
    "        hook = fc1.register_forward_hook(get_input(mlp_input_dict, i))\n",
    "        hooks.append(hook)\n",
    "        \n",
    "    # compute and store first derivative squared\n",
    "    loss = compute_acc_grad(model.cuda(), examples.cuda(), [qs, ks, vs, ds, fc1s, fc2s])\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # compute and store first derivative\n",
    "    loss = loss.mean()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Once first derivative and second derivative squared are stored,\n",
    "    # compute the importances.\n",
    "    importances_qk = compute_paired_importance(qs, ks, dim1=1, dim2=1)\n",
    "    importances_vd = compute_paired_importance(vs, ds, dim1=1, dim2=0)\n",
    "    importances_mlp = compute_paired_importance(fc1s, fc2s, dim1=1, dim2=0)\n",
    "\n",
    "    # all_imps = torch.cat([importances_qk, importances_vd, importances_mlp])\n",
    "\n",
    "    layer_to_imps = {}\n",
    "    layers = model.get_submodule(\"model\").get_submodule(\"layers\")\n",
    "    for i, layer in enumerate(layers):\n",
    "        layer_to_imps[layer] = [importances_qk[i,:], importances_vd[i,:], importances_mlp[i,:]]\n",
    "        \n",
    "\n",
    "    # cleanup\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    sample_inputs_trailing_token = get_sample_input_trailing_token(mlp_input_dict)\n",
    "    \n",
    "    return layer_to_imps, sample_inputs_trailing_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples not defined\n",
    "\n",
    "all_imps, sample_inputs_trailing_token = compute_delta_loss_importances(model, examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pruned_weights(shape0, shape1, dtype, weightdata, biasdata=None):\n",
    "    layer_pruned = torch.nn.Linear(\n",
    "        shape0,\n",
    "        shape1,\n",
    "        dtype=dtype\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():    \n",
    "        layer_pruned.weight.data = weightdata\n",
    "        if biasdata != None:\n",
    "            layer_pruned.bias.data = biasdata\n",
    "\n",
    "    return layer_pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def full_pruning(layer_to_imps, prune_ratio):\n",
    "    \"\"\" Given a dictionary of layer -> [3x importance] tensor (for query-key, value-dense, and mlp), prunes\n",
    "    the full model.\n",
    "    \"\"\"\n",
    "\n",
    "    all_imps = []\n",
    "\n",
    "    for importances in layer_to_imps.values():\n",
    "        concatenated = importances[0] + importances[1] + importances[2]\n",
    "        all_imps.extend(concatenated)\n",
    "    \n",
    "    num_prune_cells = int(len(all_imps) * prune_ratio)\n",
    "    \n",
    "    # Choose which node-indexes to prune, mark those indexes with '0'\n",
    "    _, indices_to_replace = torch.topk(all_imps, num_prune_cells, largest=False)\n",
    "    mask = torch.ones_like(all_imps, dtype=torch.bool)\n",
    "    mask[indices_to_replace] = False\n",
    "    \n",
    "    # Make a new dict with indexes with smallest values zeroed out\n",
    "    split_size_layer = len(concatenated)\n",
    "    split1 = len(list(layer_to_imps.values())[0][0])\n",
    "    split2 = len(list(layer_to_imps.values())[0][1])\n",
    "\n",
    "    layer_split_mask = torch.split(mask, split_size_layer)\n",
    "    layer_module_split_mask = [[layer[:split1], layer[split1:split2], layer[split2:]] for layer in layer_split_mask]\n",
    "    mask_dict = {key: value for key, value in zip(importances.keys(), layer_module_split_mask)}\n",
    "\n",
    "    # Prune each mlp\n",
    "    for layer, mask_list in mask_dict.items():\n",
    "        keep_idx_qk = torch.arange(mask_list[0].shape[0], dtype=torch.long)[mask_list[0]]\n",
    "        keep_idx_vd = torch.arange(mask_list[1].shape[0], dtype=torch.long)[mask_list[1]]\n",
    "        keep_idx_mlp = torch.arange(mask_list[2].shape[0], dtype=torch.long)[mask_list[2]]\n",
    "\n",
    "        attn = layer.get_submodule(\"self_attn\")\n",
    "        mlp = layer.get_submodule(\"mlp\")\n",
    "        query = attn.q_proj\n",
    "        key = attn.k_proj\n",
    "        value = attn.v_proj\n",
    "        dense = attn.dense\n",
    "        fc1 = mlp.fc1\n",
    "        fc2 = mlp.fc2\n",
    "\n",
    "        query_pruned = create_pruned_weights(\n",
    "            query.weight.shape[1],\n",
    "            keep_idx_qk.shape[0],\n",
    "            query.weight.dtype,\n",
    "            torch.clone(query.weight[keep_idx_qk])\n",
    "            )\n",
    "        \n",
    "        key_pruned = create_pruned_weights(\n",
    "            key.weight.shape[1],\n",
    "            keep_idx_qk.shape[0],\n",
    "            query.weight.dtype,\n",
    "            torch.clone(query.weight[keep_idx_qk])\n",
    "            )\n",
    "        \n",
    "        value_pruned = create_pruned_weights(\n",
    "            value.weight.shape[1],\n",
    "            keep_idx_vd.shape[0],\n",
    "            value.weight.dtype,\n",
    "            torch.clone(query.weight[keep_idx_vd])\n",
    "            )\n",
    "        \n",
    "        dense_pruned = create_pruned_weights(\n",
    "            keep_idx_vd.shape[0],\n",
    "            dense.weight.shape[0],\n",
    "            dense.weight.dtype,\n",
    "            torch.clone(dense.weight[:, keep_idx_vd])\n",
    "            )\n",
    "        \n",
    "        fc1_pruned = create_pruned_weights(\n",
    "            fc1.weight.shape[1],\n",
    "            keep_idx_mlp.shape[0],\n",
    "            fc1.weight.dtype,\n",
    "            torch.clone(fc1.weight[keep_idx_mlp]),\n",
    "            torch.clone(fc1.bias[keep_idx_mlp])\n",
    "        )\n",
    "\n",
    "        fc2_pruned = create_pruned_weights(\n",
    "            keep_idx_mlp.shape[0],\n",
    "            fc2.weight.shape[0],\n",
    "            fc2.weight.dtype,\n",
    "            torch.clone(fc2.weight[:, keep_idx_mlp]),\n",
    "            torch.clone(fc2.bias[:, keep_idx_mlp])\n",
    "        )\n",
    "\n",
    "        query = query_pruned\n",
    "        key = key_pruned\n",
    "        value = value_pruned\n",
    "        dense = dense_pruned\n",
    "        fc1 = fc1_pruned\n",
    "        fc2 = fc2_pruned"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
