{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d7fafc9-c2da-408a-ad06-b4dc9dcdb720",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c7dbe4-0c96-4138-adbc-7fc5c7b3e994",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d024b253-a337-45d0-a823-131d38617ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ff117d-a0db-4c44-9bab-4ebcd6877b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from dataset_preprocessing import TokenInfo\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77093fe6-1ebb-44b1-b8ce-e42e3e3d8ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_r(prune_ratio):\n",
    "    gamma=prune_ratio\n",
    "    h = 8192\n",
    "    i = 2048\n",
    "    keep_cost = 4*(1-gamma)*h*i + 2*(1-gamma)*h + i\n",
    "    reg_cost = 4*0.5*h*i + 2*0.5*h + i\n",
    "\n",
    "    r = (reg_cost - keep_cost - (2*gamma*h + i)) / (4*(gamma*h + i))\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13b4506c-082a-4100-b235-c6826a5219a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRUNE_RATIO = 0.72\n",
    "RANK = int(get_r(PRUNE_RATIO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1b27cf3-07ac-4096-87a8-66de22a95a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.72, 464)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRUNE_RATIO, RANK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad5cdcc-7d6a-4beb-836f-badf547df019",
   "metadata": {},
   "source": [
    "# Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7f2fb6-cf16-4cd6-994a-81d50c032656",
   "metadata": {},
   "source": [
    "## Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb3f5727-4779-49f5-b4ef-6ead12455a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_importances():\n",
    "    # print(\"this is wrong\")\n",
    "    dir = \"./new_importances_data2\" # pick the right dir\n",
    "    imp_files = os.listdir(dir)\n",
    "    imp_files = [file for file in imp_files if file.endswith(\".pkl\")]\n",
    "    importances = {}\n",
    "    for imp_file in tqdm(imp_files):\n",
    "        importances.update(pd.read_pickle(f\"{dir}/{imp_file}\"))\n",
    "    return importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c336ee5b-8ec4-4dd8-abdd-213adf62fa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_imporances(importances):\n",
    "    avg_imps = [torch.zeros_like(imp) for imp in list(importances.values())[0][0]]\n",
    "    ttl = sum(token[2] for token in importances.keys())\n",
    "    for token, imps in tqdm(importances.items()):\n",
    "        imps = imps[0]\n",
    "        for i, layer_imps in enumerate(imps):\n",
    "            avg_imps[i] += layer_imps * (token[2] / ttl)\n",
    "    return avg_imps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57a3618e-7c28-4dea-97a6-40a0fc3e2cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 49700/49700 [00:56<00:00, 880.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# avg_importances = get_avg_imporances(imps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cfbc11bc-f115-4a46-86c0-0fecfc51289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.to_pickle(avg_importances, \"./new_weighted_avg_importances.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49e8f8c-bb58-4907-886a-f9967d82dab2",
   "metadata": {},
   "source": [
    "## Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab26a3da-5548-4088-b927-0b6d10d25976",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_importances = pd.read_pickle(\"./avg_importances.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b3cda5-fe19-4e85-83b4-3a29af8f7f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(avg_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef309891-beb4-41c7-9596-302f92d59443",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17e3e52-b6d9-44e9-8520-3bc1b6077c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"microsoft/phi-1_5\"\n",
    "model_revision = \"349cf8b5e81fd5f791d1740da5de1313a0419bbd\" # latest as of feb 1st\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    revision=model_revision,\n",
    "    trust_remote_code=True,\n",
    "    # be careful with this?\n",
    "    # torch_dtype=torch.float16,\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11df3a0b-3c76-41f4-87c6-9df43fda7aa2",
   "metadata": {},
   "source": [
    "# Low rank matrix initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b52bea3-e660-4ad5-b3a8-7bd1ade56d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_mlp_loras_simple(mlp, most_imp_cells, rank=64):\n",
    "    print(\"get_mlp_loras_simple\")\n",
    "    # assumes descending order of importances in most_imp_cells\n",
    "    fc1_mat = mlp.fc1.weight[most_imp_cells[:rank]]\n",
    "    fc1_bias = mlp.fc1.bias[most_imp_cells] # bias is full dimension\n",
    "    fc2_mat = mlp.fc2.weight[:, most_imp_cells[:rank]]\n",
    "    # no need of fc2 bias\n",
    "    \n",
    "    fc1_A = fc1_mat\n",
    "    fc1_B = torch.concat((torch.eye(rank), torch.zeros(len(most_imp_cells) - rank, rank)), dim=0)\n",
    "    \n",
    "    fc2_B = fc2_mat\n",
    "    fc2_A = torch.concat((torch.eye(rank), torch.zeros(rank, len(most_imp_cells) - rank)), dim=1)\n",
    "    # fc2_A = fc2_mat\n",
    "    # fc2_B = torch.concat((torch.eye(rank), torch.zeros(mlp.fc2.out_features - rank, rank)), dim=0)\n",
    "    return fc1_A.clone(), fc1_B.clone(), fc1_bias.clone(), fc2_B.clone(), fc2_A.clone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c7d132-be3e-4d9b-a9d0-65a67c562567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "class SVDRes():\n",
    "    def __init__(self, U, S, V):\n",
    "        self.U = U\n",
    "        self.S = S\n",
    "        self.V = V\n",
    "\n",
    "def get_svd(tens):\n",
    "    # print(\"running svd\")\n",
    "    tens = tens.cuda()\n",
    "    res = torch.svd(tens)\n",
    "    return SVDRes(res.U.cpu(), res.S.cpu(), res.V.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea249ac-b7b4-4232-b1c7-66b038386717",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_mlp_loras_svd(mlp, most_imp_cells, rank=64, init_bias=True):\n",
    "    print(\"get_mlp_loras_svd\")\n",
    "    # assumes descending order of importances in most_imp_cells\n",
    "    fc1_mat = mlp.fc1.weight[most_imp_cells]\n",
    "    if init_bias:\n",
    "        fc1_bias = mlp.fc1.bias[most_imp_cells]\n",
    "    else:\n",
    "        assert False, \"not implemented yet\"\n",
    "    fc2_mat = mlp.fc2.weight[:, most_imp_cells]\n",
    "    # no need of fc2 bias\n",
    "\n",
    "    fc1_svd = get_svd(fc1_mat)\n",
    "    fc2_svd = get_svd(fc2_mat)\n",
    "    \n",
    "    fc1_B = fc1_svd.U[:, :rank] @ torch.diag(fc1_svd.S[:rank])\n",
    "    fc1_A = fc1_svd.V.T[:rank]\n",
    "\n",
    "    fc2_B = fc2_svd.U[:, :rank] @ torch.diag(fc2_svd.S[:rank])\n",
    "    fc2_A = fc2_svd.V.T[:rank]\n",
    "    return fc1_A.clone(), fc1_B.clone(), fc1_bias.clone(), fc2_B.clone(), fc2_A.clone()\n",
    "\n",
    "orig_model = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3441c0-5d26-4b7c-bedc-35feb4dc3f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def initialize_loras(orig_model, model, init_cells, lora_func=get_mlp_loras_simple):\n",
    "    for layer_i, (orig_layer, layer) in tqdm(enumerate(zip(orig_model.model.layers, model.model.layers))):\n",
    "        num_experts = len(init_cells[layer_i])\n",
    "        for expert_i in range(num_experts): # \n",
    "            most_imp_cells = init_cells[layer_i][expert_i]\n",
    "            fc1_A, fc1_B, fc1_bias, fc2_B, fc2_A = lora_func(orig_layer.mlp, most_imp_cells, RANK)\n",
    "\n",
    "            # import pdb; pdb.set_trace()\n",
    "            layer.mlp.experts_fc1[expert_i].orig_lora.lora_A.default.weight.data = fc1_A\n",
    "            layer.mlp.experts_fc1[expert_i].orig_lora.lora_B.default.weight.data = fc1_B\n",
    "            layer.mlp.experts_fc1[expert_i].lora_bias.data = fc1_bias\n",
    "            \n",
    "            layer.mlp.experts_fc2[expert_i].orig_lora.lora_A.default.weight.data = fc2_A\n",
    "            layer.mlp.experts_fc2[expert_i].orig_lora.lora_B.default.weight.data = fc2_B\n",
    "            # no bias needed for fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb561a7c-d330-47e7-9e7f-d890c2c6c7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prunners import prune_mlps_individually\n",
    "from importances import get_mlps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753dab2e-606d-4b41-b98d-9e7f38cc6012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa46797b-0549-44bc-9b4a-8fe1c9a1f36a",
   "metadata": {},
   "source": [
    "# Prune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e72d01f-2898-4cc7-a4f5-034e3c9f3f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlps = get_mlps(model)\n",
    "avg_importances = dict(zip(mlps, avg_importances))\n",
    "pruned_cells = prune_mlps_individually(avg_importances, PRUNE_RATIO)\n",
    "\n",
    "# svd init cells per expert (only 1 expert in this case)\n",
    "svd_init_cells = [[p] for p in pruned_cells]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c01ae66-bba5-4171-9269-06446ea9e4c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6b39f0b-f193-4717-a2b7-dc7aba63d908",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15da423-3465-4048-beb5-1f5aa761277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from other_datasets import get_minipile, get_c4, get_wikitext2_filtered, get_bookcorpus, get_alpaca, QADataCollator, to_dataset\n",
    "from dataset import get_baseline_dataset\n",
    "tiny_text = get_baseline_dataset()\n",
    "alpaca = get_alpaca(tokenizer, n=2000, do_split=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d4b651-21f1-4fa3-9901-c95300452f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fe6095e-3512-4132-83c2-742c46115362",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ef0abb-79c9-4728-927a-2d91e1ee2a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "from evaluation import evaluate_on_nlp_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3248b233-33fb-46a1-a03d-c68503fa94cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccEvalCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.last_step=-1\n",
    "\n",
    "    def on_evaluate(self, args, state, control, model, **kwargs):\n",
    "        if state.global_step == self.last_step:\n",
    "            return\n",
    "        self.last_step = state.global_step\n",
    "        train = model.training\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
    "            eval_res = evaluate_on_nlp_tasks(model, tokenizer, limit=100, do_shuffle=True)[\"results\"]\n",
    "            # import pdb; pdb.set_trace()\n",
    "            eval_res = {k:v[\"acc,none\"] for k,v in eval_res.items()}\n",
    "            for k, v in eval_res.items():\n",
    "                state.log_history.append(\n",
    "                    {\n",
    "                        k:v,\n",
    "                        \"epoch\":state.epoch,\n",
    "                        \"step\":state.global_step,\n",
    "                    }\n",
    "                )\n",
    "            del os.environ['TQDM_DISABLE']\n",
    "            print(eval_res)\n",
    "        model.train(train)\n",
    "\n",
    "class SaveCallback(TrainerCallback):\n",
    "    def __init__(self, save_path):\n",
    "        super().__init__()\n",
    "        self.save_path = save_path\n",
    "        self.last_step=-1\n",
    "\n",
    "    def on_evaluate(self, args, state, control, model, **kwargs):\n",
    "        if state.global_step == self.last_step:\n",
    "            return\n",
    "        self.last_step = state.global_step\n",
    "        try:\n",
    "            torch.save(model.state_dict(), self.save_path)\n",
    "        except Exception as e:\n",
    "            print(f\"error saving {e}\")\n",
    "\n",
    "class EnableMLPBias(TrainerCallback):\n",
    "    def on_init_end(self, args, state, control, model, **kwargs):\n",
    "        for n, p in model.named_parameters():\n",
    "            if \"base_layer\" in n and \"bias\" in n:\n",
    "                p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c1bd9e-0d7d-4600-a1b3-d85a196dd1b9",
   "metadata": {},
   "source": [
    "# Replace modules and init them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c00bea-6d3c-4bbf-aa4d-7b2141fd0ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experts import Experts, EmbeddingTokenIdxTracker, mark_adapters_and_routers_as_trainable, prepare_as_if_peft_model, prepare_model_for_gradient_checkpointing\n",
    "from importances import get_mlps\n",
    "from post_training import get_lora_config, get_training_arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7515d6d0-0ab2-4ec9-a7b2-d6d33ce35517",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Using rank: {RANK}\")\n",
    "lora_config = get_lora_config(r=RANK)\n",
    "training_arguments = get_training_arguments(\"./tmp\")\n",
    "\n",
    "training_arguments = prepare_as_if_peft_model(model, training_arguments, lora_config)\n",
    "\n",
    "def get_layers(model):\n",
    "    return model.get_submodule(\"model\").get_submodule(\"layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa9fdc9-2d07-49f2-b4e0-e27398e45f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = get_layers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d46821-87a1-4b1e-a596-36d3e55e03f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(layers):\n",
    "    layer.mlp = Experts(\n",
    "        model,\n",
    "        layer.mlp,\n",
    "        lora_config,\n",
    "        i,\n",
    "        layer.mlp.config,\n",
    "        num_experts=1,\n",
    "        cluster_init_router=False, # do not initialize mlp router\n",
    "        use_improved_lora=True,\n",
    "        lora_at_base_improved_lora=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b151b41-b985-4044-82fb-2e7a1afcf6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_loras(orig_model, model, init_cells=svd_init_cells, lora_func=get_mlp_loras_svd)\n",
    "mark_adapters_and_routers_as_trainable(model, lora_config)\n",
    "prepare_model_for_gradient_checkpointing(model)\n",
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ce2d66-62d5-4021-8ab2-13b43b4bcf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b5eb13-f420-41e7-999e-adf3f376840a",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f7efd1-82f6-4ce3-a8e2-fe1c2bd07344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from post_training import get_lora_config, get_training_arguments\n",
    "from dataset import get_baseline_dataset\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "from other_datasets import SFTTrainer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6fc5ae-0477-4985-b5fc-0c9676a29300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model for training\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5a7bff-25c9-4de7-b60b-baca551184e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data, eval_data = minipile[\"train\"], minipile[\"test\"]\n",
    "train_data, eval_data = tiny_text[\"train\"], tiny_text[\"test\"]\n",
    "eval_datasets = {\n",
    "    \"tiny_text\":eval_data,\n",
    "    \"alpaca\":alpaca,\n",
    "    # \"minipile\":minipile,\n",
    "    # \"c4\":c4,\n",
    "    # \"wikitext\":wikitext,\n",
    "    # \"tiny_text\":tiny_text,\n",
    "    # \"bookcorpus\":bookcorpus,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16980315-3fdc-46cb-b830-32f0ff2f3d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = \"./tmp/weighted_routing_better_lora_svd_init_moe50_model_state_dict\"\n",
    "callbacks = [AccEvalCallback(), EnableMLPBias()] #, SaveCallback(save_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594fcc2b-92e3-4981-bf8c-6f84f2e42da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "training_arguments.save_strategy=\"no\"\n",
    "training_arguments.eval_steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c46ed30-7d65-482b-be0e-31ce76286369",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer_(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_datasets[\"tiny_text\"],\n",
    "    # peft_config=lora_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=1024, # tweak this\n",
    "    # TODO: think harder about the datacollator\n",
    "    # data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "    #     tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "    # ),\n",
    "    callbacks=callbacks,\n",
    "    data_collator=QADataCollator(tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b98a51-4c4d-496f-80c8-0e77cf980775",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de99ae00-22f2-4388-8d06-81fe1d88a967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CodeGenTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1301' max='1666' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1301/1666 6:05:54 < 1:42:48, 0.06 it/s, Epoch 1.56/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.548600</td>\n",
       "      <td>3.477710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.422100</td>\n",
       "      <td>3.383874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.414300</td>\n",
       "      <td>3.342912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.308200</td>\n",
       "      <td>3.306208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.257300</td>\n",
       "      <td>3.284032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.329100</td>\n",
       "      <td>3.263538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.256900</td>\n",
       "      <td>3.249590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.313500</td>\n",
       "      <td>3.234140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.053100</td>\n",
       "      <td>3.227070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.044700</td>\n",
       "      <td>3.220730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>3.071400</td>\n",
       "      <td>3.211296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.024700</td>\n",
       "      <td>3.205665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 02:58]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28:21:07:27,717 WARNING  [huggingface.py:105] `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "2024-03-28:21:07:27,738 WARNING  [huggingface.py:315] Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will shuffle dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/research/robgarct/.conda/envs/cs224n-pip3/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "100%|█| 1000/1000 [01:04<00:00, 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hellaswag': 0.32, 'piqa': 0.69, 'boolq': 0.62, 'winogrande': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any parent up to mount point /)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will shuffle dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/research/robgarct/.conda/envs/cs224n-pip3/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "100%|█| 1000/1000 [00:59<00:00, 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hellaswag': 0.34, 'piqa': 0.69, 'boolq': 0.6, 'winogrande': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any parent up to mount point /)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will shuffle dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/research/robgarct/.conda/envs/cs224n-pip3/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "100%|█| 1000/1000 [01:00<00:00, 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hellaswag': 0.32, 'piqa': 0.71, 'boolq': 0.62, 'winogrande': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any parent up to mount point /)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will shuffle dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/research/robgarct/.conda/envs/cs224n-pip3/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      " 16%|▏| 162/1000 [00:09<00:50, 16IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will shuffle dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/research/robgarct/.conda/envs/cs224n-pip3/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "100%|█| 1000/1000 [00:59<00:00, 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hellaswag': 0.37, 'piqa': 0.71, 'boolq': 0.61, 'winogrande': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any parent up to mount point /)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will shuffle dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/research/robgarct/.conda/envs/cs224n-pip3/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for winogrande contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/winogrande\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/research/robgarct/.conda/envs/cs224n-pip3/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "100%|█| 1000/1000 [00:59<00:00, 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hellaswag': 0.36, 'piqa': 0.7, 'boolq': 0.62, 'winogrande': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any parent up to mount point /)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will shuffle dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/research/robgarct/.conda/envs/cs224n-pip3/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "100%|█| 1000/1000 [00:59<00:00, 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hellaswag': 0.34, 'piqa': 0.74, 'boolq': 0.62, 'winogrande': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any parent up to mount point /)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will shuffle dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/research/robgarct/.conda/envs/cs224n-pip3/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "100%|█| 1000/1000 [00:59<00:00, 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hellaswag': 0.36, 'piqa': 0.72, 'boolq': 0.62, 'winogrande': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any parent up to mount point /)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will shuffle dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/research/robgarct/.conda/envs/cs224n-pip3/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "100%|█| 1000/1000 [00:59<00:00, 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hellaswag': 0.37, 'piqa': 0.72, 'boolq': 0.6, 'winogrande': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any parent up to mount point /)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will shuffle dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/research/robgarct/.conda/envs/cs224n-pip3/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "100%|█| 1000/1000 [00:59<00:00, 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hellaswag': 0.36, 'piqa': 0.72, 'boolq': 0.58, 'winogrande': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any parent up to mount point /)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will shuffle dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/research/robgarct/.conda/envs/cs224n-pip3/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "100%|█| 1000/1000 [00:59<00:00, 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hellaswag': 0.37, 'piqa': 0.72, 'boolq': 0.59, 'winogrande': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any parent up to mount point /)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will shuffle dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/research/robgarct/.conda/envs/cs224n-pip3/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      " 24%|▏| 236/1000 [00:14<00:45, 16IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "288848f4-1dc1-49c2-8535-915e7d39a45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_state = trainer.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb9dbdc9-ef9b-4e8e-99ef-d31ba29bcacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loss = pd.DataFrame(trainer_state.log_history)[[\"step\", \"eval_loss\"]].set_index(\"step\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bb7a41b-f5de-4f57-b937-2ecded295749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>step</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>3.477710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>3.383874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>3.342912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>3.306208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>3.284032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>3.263538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>3.249590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>3.234140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>3.227070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>3.220730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>3.211296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>3.205665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>3.196675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>3.191324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>3.186852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>3.180918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      eval_loss\n",
       "step           \n",
       "100    3.477710\n",
       "200    3.383874\n",
       "300    3.342912\n",
       "400    3.306208\n",
       "500    3.284032\n",
       "600    3.263538\n",
       "700    3.249590\n",
       "800    3.234140\n",
       "900    3.227070\n",
       "1000   3.220730\n",
       "1100   3.211296\n",
       "1200   3.205665\n",
       "1300   3.196675\n",
       "1400   3.191324\n",
       "1500   3.186852\n",
       "1600   3.180918"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d867380-3054-4d42-9123-04e19166e46c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2638c063-98bc-400c-8a25-a37b2e8cb827",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1a1496-de2f-4d02-b960-3bc605a211bc",
   "metadata": {
    "id": "HYj7J7Ga0cxX"
   },
   "outputs": [],
   "source": [
    "from evaluation import evaluate_on_nlp_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9827a877-ab00-4206-ac9e-0b8c30dbc1f7",
   "metadata": {
    "id": "kkFRTLMY0cu_"
   },
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888b2a8b-ccdb-443a-b5b3-f2c52b043264",
   "metadata": {
    "id": "4XCa4Oob0csn"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    eval_res = evaluate_on_nlp_tasks(model, tokenizer, limit=300, do_shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f60b9fe-48ae-41e9-8172-7eb04c3cc91a",
   "metadata": {
    "id": "gcoKz3me0cqZ"
   },
   "outputs": [],
   "source": [
    "eval_res[\"results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7007b25-4aa1-4c5c-94b7-73fe23db4e7b",
   "metadata": {
    "id": "UIJuACQB0coM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a4775d-bc21-4154-af67-56fdb7d03b76",
   "metadata": {
    "id": "jwdG7Lhi2gZy"
   },
   "outputs": [],
   "source": [
    "eval_res_orig = evaluate_on_nlp_tasks(model, tokenizer, limit=1000, bootstrap_iters=1000, do_shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e3d8fc-1a39-475d-a562-8ebba4072004",
   "metadata": {
    "id": "MHH9Nq8v2fxj"
   },
   "outputs": [],
   "source": [
    "eval_res_orig[\"results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc279518-ef24-4b69-9ff9-979bde63d3a2",
   "metadata": {
    "id": "Tf0JfCcW0chu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df214a0-13fc-4e95-a3a7-7b635f9711f1",
   "metadata": {
    "id": "bVcMBezD0cmW"
   },
   "outputs": [],
   "source": [
    "eval_res = evaluate_on_nlp_tasks(model, tokenizer, limit=1000, bootstrap_iters=1000, do_shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4a69c0-0718-4f1a-b052-0d260515486f",
   "metadata": {
    "id": "MN9Nehnd0ckE"
   },
   "outputs": [],
   "source": [
    "eval_res[\"results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0549cc-4ee1-4fef-b673-37af8045fcf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6c721dc-2587-48ff-be37-6a8c9155d1bf",
   "metadata": {
    "id": "y-fvt2k00o3m"
   },
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d36c5c5-4dff-4dcc-9b98-c7a4aa2ea896",
   "metadata": {
    "id": "iMherTxT0ou9"
   },
   "outputs": [],
   "source": [
    "# model.cpu();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd039e66-94f1-435f-85dc-1cbd7fe0cc64",
   "metadata": {
    "id": "DNeG2dFE0ose"
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2bd9f4-c5a1-47d0-b1dc-6859d9a31026",
   "metadata": {
    "id": "qk2w5lTM0op9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24d09e18-c2a7-4e7f-a866-36259fc2f494",
   "metadata": {
    "id": "RByHgPZC0xpQ"
   },
   "source": [
    "# Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eb6900-3177-428e-9a39-154fd88c3078",
   "metadata": {
    "id": "FHmMlRa_02Ep"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(trainer_state.log_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983d5d9f-5db0-41a9-919f-b877fb1db7eb",
   "metadata": {
    "id": "mKCfP4JM02Ct"
   },
   "outputs": [],
   "source": [
    "metrics_df = df[[\"step\", \"hellaswag\", \"piqa\", \"boolq\", \"winogrande\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2918c98c-9152-484c-9cad-8d01df8727ca",
   "metadata": {
    "id": "yAwQFeiz02AS"
   },
   "outputs": [],
   "source": [
    "metrics_df[[\"step\", \"boolq\"]].dropna().set_index(\"step\").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab28801-0600-4068-88a6-3ea528b0a4e4",
   "metadata": {
    "id": "8F9ss7Hg01-K"
   },
   "outputs": [],
   "source": [
    "metrics_df[[\"step\", \"hellaswag\"]].dropna().set_index(\"step\").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1213dc4d-326c-4a76-84b7-d019d803fc26",
   "metadata": {
    "id": "1YGrsE8R0172"
   },
   "outputs": [],
   "source": [
    "metrics_df = df[[\"step\", \"piqa\"]].dropna().set_index(\"step\").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6960f44-d49a-4ce1-a8e3-a31b8e159981",
   "metadata": {
    "id": "1PvB0OVV015b"
   },
   "outputs": [],
   "source": [
    "metrics_df = df[[\"step\", \"winogrande\"]].dropna().set_index(\"step\").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cdc973-e2dc-4e0b-abf4-41d9ac92948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.layers[0].mlp.experts_fc1[0].orig_lora.lora_B.default.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63beb9e1-807b-41ad-a58b-20132ac9ae4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
